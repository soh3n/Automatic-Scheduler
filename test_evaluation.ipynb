{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the script to Evaluate Email-Label Pairs\n",
    "### Criteria:\n",
    "1. Spam: ACC, F1, Recall,\n",
    "2. Time_Sensitive: ACC, F1, Recall,\n",
    "3. Time Period ACC\n",
    "4. Type ACC\n",
    "5. Category ACC\n",
    "6. Format ACC\n",
    "7. Priority_Level ACC (Should not be so strict)\n",
    "8. Overall: weighted sum score?\n",
    "\n",
    "### Field:\n",
    "{\n",
    "\n",
    "    \"Spam\": \"Yes\" / \"No\",\n",
    "    \"Subject\": ,\n",
    "    \"Sender\": ,\n",
    "    \"send_date\": ,\n",
    "    \"Time_Sensitive\": \"Yes\" / \"No\",\n",
    "    \"Start\": ,\n",
    "    \"End\": ,\n",
    "    \"Type\": \"Event\" / \"Reminder\" / \"N/A\",\n",
    "    \"Category\": \"Work\" / \"Study\" / \"Leisure\",\n",
    "    \"Format\": \"Online\" / \"In-person\",\n",
    "    \"Location\": ,\n",
    "    \"Action_Required\": \"Yes\" / \"No\",\n",
    "    \"Priority_Level\": \"Low\" / \"Medium\" / \"High\" / \"Urgent\" \n",
    "    \n",
    "},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.evaluation_utils import evaluate_label_single, calculate_overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_time_period_correct(pred_start, pred_end, true_start, true_end):\n",
    "    \"\"\"\n",
    "    Strictly compare predicted and true time periods.\n",
    "\n",
    "    Parameters:\n",
    "        pred_start (str): Predicted start time as a string (e.g., \"2024-11-20 09:17\").\n",
    "        pred_end (str): Predicted end time as a string (e.g., \"2024-11-20 11:25\").\n",
    "        true_start (str): True start time as a string.\n",
    "        true_end (str): True end time as a string.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if both start and end times match exactly, False otherwise.\n",
    "    \"\"\"\n",
    "    # Convert strings to datetime objects\n",
    "    pred_start_dt = datetime.strptime(pred_start, \"%Y-%m-%d %H:%M\")\n",
    "    pred_end_dt = datetime.strptime(pred_end, \"%Y-%m-%d %H:%M\")\n",
    "    true_start_dt = datetime.strptime(true_start, \"%Y-%m-%d %H:%M\")\n",
    "    true_end_dt = datetime.strptime(true_end, \"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    # Check for exact match\n",
    "    return pred_start_dt == true_start_dt and pred_end_dt == true_end_dt\n",
    "\n",
    "def evaluate_label(pred, true, weights=None):\n",
    "    \"\"\"\n",
    "    Evaluate categorical fields and custom logic for Time Period and Priority Level.\n",
    "\n",
    "    Parameters:\n",
    "        pred (list of dict): Predicted labels.\n",
    "        true (list of dict): True labels.\n",
    "        weights (dict): Weights for each field except Spam.\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation results for each field and overall score.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        # Default weights (Spam is NOT included in the score)\n",
    "        weights = {\n",
    "            \"Time_Sensitive\": 0.2,\n",
    "            \"Type\": 0.15,\n",
    "            \"Category\": 0.15,\n",
    "            \"Format\": 0.1,\n",
    "            \"Time Period\": 0.2,\n",
    "            \"Priority_Level\": 0.2,\n",
    "        }\n",
    "\n",
    "    # Initialize results\n",
    "    results = {\"Field\": {}, \"Overall Weighted Score\": 0}\n",
    "    spam_correct = 1  # Default to correct unless proven otherwise\n",
    "\n",
    "    for pred_item, true_item in zip(pred, true):\n",
    "        # print(true_item)\n",
    "        for field in true_item.keys():\n",
    "            if field in pred_item:\n",
    "                if field in [\"Time_Sensitive\", \"Type\", \"Category\", \"Format\"]:\n",
    "                    # Categorical fields\n",
    "                    results[\"Field\"][field] = 1 if pred_item[field] == true_item[field] else 0\n",
    "                elif field in [\"Start\", \"End\"]:\n",
    "                    # Time Period\n",
    "                    # time_correct = (\n",
    "                    #     (pred_item[\"Start\"], pred_item[\"End\"]) == (true_item[\"Start\"], true_item[\"End\"])\n",
    "                    # )\n",
    "                    time_correct = is_time_period_correct(\n",
    "                        pred_item[\"Start\"],\n",
    "                        pred_item[\"End\"],\n",
    "                        true_item[\"Start\"],\n",
    "                        true_item[\"End\"]\n",
    "                    )\n",
    "                    results[\"Field\"][\"Time Period\"] = 1 if time_correct else 0\n",
    "                elif field == \"Priority_Level\":\n",
    "                    # Priority Level (Relaxed Match)\n",
    "                    priority_map = {\"Low\": 1, \"Medium\": 2, \"High\": 3, \"Urgent\": 4}\n",
    "                    pred_priority = priority_map[pred_item[field]]\n",
    "                    true_priority = priority_map[true_item[field]]\n",
    "                    results[\"Field\"][\"Priority_Level\"] = 1 if abs(pred_priority - true_priority) <= 1 else 0\n",
    "                elif field == \"Spam\":\n",
    "                    # Spam correctness (Denominator)\n",
    "                    spam_correct = 1 if pred_item[field] == true_item[field] else 0\n",
    "                    results[\"Field\"][\"Spam\"] = spam_correct\n",
    "\n",
    "        # Calculate weighted score using fields except Spam\n",
    "        weighted_score = sum(\n",
    "            results[\"Field\"].get(field, 0) * weights.get(field, 0)\n",
    "            for field in weights.keys()\n",
    "        )\n",
    "        results[\"Overall Weighted Score\"] = weighted_score if spam_correct == 1 else 0\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_label_single(pred, true, weights=None):\n",
    "    \"\"\"\n",
    "    Evaluate a single pair of prediction and true label.\n",
    "\n",
    "    Parameters:\n",
    "        pred (dict): Predicted label.\n",
    "        true (dict): True label.\n",
    "        weights (dict): Weights for each field except Spam.\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation results for each field and overall score.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        # Default weights (Spam is NOT included in the score)\n",
    "        weights = {\n",
    "            \"Time_Sensitive\": 0.2,\n",
    "            \"Type\": 0.15,\n",
    "            \"Category\": 0.15,\n",
    "            \"Format\": 0.1,\n",
    "            \"Time Period\": 0.2,\n",
    "            \"Priority_Level\": 0.2,\n",
    "        }\n",
    "\n",
    "    # Initialize results\n",
    "    results = {\"Field\": {}, \"Overall Weighted Score\": 0}\n",
    "\n",
    "    # Spam correctness (Denominator)\n",
    "    spam_correct = 1 if pred[\"Spam\"] == true[\"Spam\"] else 0\n",
    "    results[\"Field\"][\"Spam\"] = spam_correct\n",
    "\n",
    "    # Evaluate fields\n",
    "    for field in true.keys():\n",
    "        if field in [\"Time_Sensitive\", \"Type\", \"Category\", \"Format\"]:\n",
    "            # Categorical fields\n",
    "            results[\"Field\"][field] = 1 if pred[field] == true[field] else 0\n",
    "        elif field in [\"Start\", \"End\"]:\n",
    "            # Time Period\n",
    "            time_correct = is_time_period_correct(\n",
    "                pred[\"Start\"], pred[\"End\"], true[\"Start\"], true[\"End\"]\n",
    "            )\n",
    "            results[\"Field\"][\"Time Period\"] = 1 if time_correct else 0\n",
    "        elif field == \"Priority_Level\":\n",
    "            # Priority Level (Relaxed Match)\n",
    "            priority_map = {\"Low\": 1, \"Medium\": 2, \"High\": 3, \"Urgent\": 4}\n",
    "            pred_priority = priority_map[pred[field]]\n",
    "            true_priority = priority_map[true[field]]\n",
    "            results[\"Field\"][\"Priority_Level\"] = 1 if abs(pred_priority - true_priority) <= 1 else 0\n",
    "\n",
    "    # Calculate weighted score using fields except Spam\n",
    "    weighted_score = sum(\n",
    "        results[\"Field\"].get(field, 0) * weights.get(field, 0)\n",
    "        for field in weights.keys()\n",
    "    )\n",
    "    results[\"Overall Weighted Score\"] = weighted_score if spam_correct == 1 else 0\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_metrics(results_list):\n",
    "    \"\"\"\n",
    "    Calculate overall ACC, F1, and Recall for binary fields, ACC for categorical fields,\n",
    "    and an averaged weighted score.\n",
    "\n",
    "    Parameters:\n",
    "        results_list (list of dict): List of evaluation results for individual predictions.\n",
    "\n",
    "    Returns:\n",
    "        dict: Overall metrics for binary fields, categorical fields, and average weighted score.\n",
    "    \"\"\"\n",
    "    # Initialize storage\n",
    "    binary_fields = [\"Spam\", \"Time_Sensitive\"]\n",
    "    categorical_fields = [\"Time Period\", \"Type\", \"Category\", \"Format\", \"Priority_Level\"]\n",
    "\n",
    "    binary_true = {field: [] for field in binary_fields}\n",
    "    binary_pred = {field: [] for field in binary_fields}\n",
    "\n",
    "    categorical_correct = {field: 0 for field in categorical_fields}\n",
    "    categorical_total = {field: 0 for field in categorical_fields}\n",
    "\n",
    "    total_weighted_score = 0\n",
    "    num_results = len(results_list)\n",
    "\n",
    "    # Aggregate results\n",
    "    for result in results_list:\n",
    "        total_weighted_score += result[\"Overall Weighted Score\"]\n",
    "        for field in result[\"Field\"]:\n",
    "            if field in binary_fields:\n",
    "                binary_true[field].append(1)  # True value is always \"correct\"\n",
    "                binary_pred[field].append(result[\"Field\"][field])  # Append prediction (0 or 1)\n",
    "            elif field in categorical_fields:\n",
    "                categorical_total[field] += 1\n",
    "                if result[\"Field\"][field] == 1:\n",
    "                    categorical_correct[field] += 1\n",
    "\n",
    "    # Calculate metrics for binary fields\n",
    "    binary_metrics = {}\n",
    "    for field in binary_fields:\n",
    "        binary_metrics[field] = {\n",
    "            \"ACC\": accuracy_score(binary_true[field], binary_pred[field]),\n",
    "            \"F1\": f1_score(binary_true[field], binary_pred[field]),\n",
    "            \"Recall\": recall_score(binary_true[field], binary_pred[field]),\n",
    "        }\n",
    "\n",
    "    # Calculate accuracy for categorical fields\n",
    "    categorical_metrics = {\n",
    "        field: categorical_correct[field] / categorical_total[field]\n",
    "        for field in categorical_fields\n",
    "    }\n",
    "\n",
    "    # Calculate averaged weighted score\n",
    "    averaged_weighted_score = total_weighted_score / num_results\n",
    "\n",
    "    return {\n",
    "        \"Binary Metrics\": binary_metrics,\n",
    "        \"Categorical Metrics\": categorical_metrics,\n",
    "        \"Averaged Weighted Score\": averaged_weighted_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = \"./data/test_examples/test_pred.json\"\n",
    "true_path = \"./data/test_examples/test_true.json\"\n",
    "with open(pred_path, 'r') as file:\n",
    "    pred_labels = json.load(file)\n",
    "with open(true_path, 'r') as file:\n",
    "    true_labels = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "assert len(pred_labels) == len(true_labels)\n",
    "for i in range(len(pred_labels)):\n",
    "    results.append(evaluate_label_single(pred_labels[i], true_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Field': {'Spam': 1,\n",
       "   'Time_Sensitive': 1,\n",
       "   'Time Period': 0,\n",
       "   'Type': 1,\n",
       "   'Category': 1,\n",
       "   'Format': 1,\n",
       "   'Priority_Level': 1},\n",
       "  'Overall Weighted Score': 0.8},\n",
       " {'Field': {'Spam': 1,\n",
       "   'Time_Sensitive': 1,\n",
       "   'Time Period': 1,\n",
       "   'Type': 1,\n",
       "   'Category': 1,\n",
       "   'Format': 1,\n",
       "   'Priority_Level': 1},\n",
       "  'Overall Weighted Score': 1.0},\n",
       " {'Field': {'Spam': 1,\n",
       "   'Time_Sensitive': 1,\n",
       "   'Time Period': 1,\n",
       "   'Type': 1,\n",
       "   'Category': 1,\n",
       "   'Format': 1,\n",
       "   'Priority_Level': 1},\n",
       "  'Overall Weighted Score': 1.0},\n",
       " {'Field': {'Spam': 0,\n",
       "   'Time_Sensitive': 1,\n",
       "   'Time Period': 1,\n",
       "   'Type': 1,\n",
       "   'Category': 1,\n",
       "   'Format': 1,\n",
       "   'Priority_Level': 1},\n",
       "  'Overall Weighted Score': 0},\n",
       " {'Field': {'Spam': 1,\n",
       "   'Time_Sensitive': 1,\n",
       "   'Time Period': 1,\n",
       "   'Type': 1,\n",
       "   'Category': 1,\n",
       "   'Format': 1,\n",
       "   'Priority_Level': 1},\n",
       "  'Overall Weighted Score': 1.0}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Binary Metrics': {'Spam': {'ACC': 0.8,\n",
       "   'F1': 0.8888888888888888,\n",
       "   'Recall': 0.8},\n",
       "  'Time_Sensitive': {'ACC': 1.0, 'F1': 1.0, 'Recall': 1.0}},\n",
       " 'Categorical Metrics': {'Time Period': 0.8,\n",
       "  'Type': 1.0,\n",
       "  'Category': 1.0,\n",
       "  'Format': 1.0,\n",
       "  'Priority_Level': 1.0},\n",
       " 'Averaged Weighted Score': 0.76}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = calculate_overall_metrics(results)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
